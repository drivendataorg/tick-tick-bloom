{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2b1a8e",
   "metadata": {},
   "source": [
    "# Modeling Notes for algae bloom competition\n",
    "\n",
    "By [Andrew Wheeler](mailto:apwheele@gmail.com), [Personal Blog to see my work](https://andrewpwheeler.com/)\n",
    "\n",
    "These are my notes for the data competition, [Tick Tick Bloom: Harmful Algal Bloom Detection Challenge](https://www.drivendata.org/competitions/143/tick-tick-bloom/page/649/).\n",
    "\n",
    "For a high level, I ensemble three different boosted models in the final solution:\n",
    "\n",
    "  - xgboost with fields `region, cluster, date`\n",
    "  - catboost with fields `region, cluster, date, latitude, longitude, maxe, dife`\n",
    "  - lightboost with fields `region, cluster, imtype, date, latitude, longitude , elevation ,dife, imtype, prop_lake_2500, r_2500, g_2500, b_2500, prop_lake_1000, r_1000, g_1000, b_1000`\n",
    "\n",
    "For a description of the variable fields:\n",
    "\n",
    "  - region: metadata given region of US\n",
    "  - cluster: arbitrary binning of lat/lon into different clusters (based on looking at distribution of lat/lon)\n",
    "  - date: date field for event, the models under the hood turn these into features of days since 1/1/2015, day of the week, and month of the year (experimented with year as well, but not in final model)\n",
    "  - latitude: given via metadata\n",
    "  - latitude: given via metadata\n",
    "  - maxe: maximum elevation within 1000 meters of lat/lon (from DEM)\n",
    "  - dife: difference in min/max elevation of 1000 meters (from DEM)\n",
    "  - elevation: elevation at exact lat/lon (from DEM)\n",
    "  - imtype: image type (ended up only using Sentinel images, filtered out landsat entirely)\n",
    "  - prop_lake_2500: Estimate of water area at 2500 meters from lat/lon\n",
    "  - r_2500: estimate of red inside water area at 2500 meters\n",
    "  - g_2500: estimate of green inside water area at 2500 meters\n",
    "  - b_2500: estimate of blue inside water area at 2500 meters\n",
    "  - prop_lake_1000: Estimate of water area at 1000 meters from lat/lon\n",
    "  - r_1000: estimate of red inside water area at 1000 meters\n",
    "  - g_1000: estimate of green inside water area at 1000 meters\n",
    "  - b_1000: estimate of blue inside water area at 1000 meters \n",
    "  \n",
    "\n",
    "In the competition, I started with each individual model, doing slight hyper-parameter tuning and seeing the optimal results on the leaderboad. Then I additionally tinkered with the final results (minor things, like tweaking the iterations and the depth of the trees).\n",
    "\n",
    "I did not bother to download the weather data (just to download the satellite data was quite a chore). For future competitions I would suggest DataDriven provide a set of already produced functions, so it is easier for participants to understand what data is valid, and make it easier for people to participate. While I imagine weather data would improve the results, I did not have time to fully understand that data and incorporate it into my model.\n",
    "\n",
    "Also note, given the error metric is mean-squared-error, I use a regression models, not multinomial. I then predict a continuous outcome, and then round/clip to the nearest integer value. For the ensemble models, I take a simple average of the not rounded values, and then round the final result.\n",
    "\n",
    "For the satellite imagery, I used [k-means image segmentation](https://docs.opencv.org/3.4/d1/d5c/tutorial_py_kmeans_opencv.html) to try to identify water areas (I doubt this works very well, the other model features did most of the work), and then calculate the R/G/B stats, as well as the size of the water area. The final models I chose within 1000 meters and 2500 meters doing this, but also tested within 500 meters as well.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "I initially made a mistake, and used sentinel data + landsat-7. The solution on 2/6 (private score 0.7580) incorrectly used the landsat-7 data. So my 2nd best solution on 2/16 (private score 0.7616) is what I illustrate here.\n",
    "\n",
    "It is a stupid mistake and I apologize. For those wishing to replicate the same results but with LandSat-8 data, you just need to change 1 character in `src/sat_fe.py` on line 89.\n",
    "\n",
    "**END NOTE**\n",
    "\n",
    "I think it would be better to just use [NOAA's list of lakes to filter the imagery](https://community.drivendata.org/t/outside-data-sources/8325/2), but that was against the competition rules. (Smaller lakes have higher concentrations.) Also this model will be bad if you submit somewhere with no lake to begin with (it will spit out numbers, but obviously they should be no risk of bloom). Using the lakes data would limit where it makes sense to even make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0d87b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>elevation</th>\n",
       "      <th>mine</th>\n",
       "      <th>maxe</th>\n",
       "      <th>...</th>\n",
       "      <th>r_1000</th>\n",
       "      <th>g_1000</th>\n",
       "      <th>b_1000</th>\n",
       "      <th>prop_lake_2500</th>\n",
       "      <th>r_2500</th>\n",
       "      <th>g_2500</th>\n",
       "      <th>b_2500</th>\n",
       "      <th>cluster</th>\n",
       "      <th>logDensity</th>\n",
       "      <th>split_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabm</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>585.0</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>162.775070</td>\n",
       "      <td>245.746124</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>6.371612</td>\n",
       "      <td>0.026089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aacd</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>290.0</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>112.367500</td>\n",
       "      <td>82.372543</td>\n",
       "      <td>124.047012</td>\n",
       "      <td>...</td>\n",
       "      <td>167.504627</td>\n",
       "      <td>155.254219</td>\n",
       "      <td>135.638269</td>\n",
       "      <td>0.160221</td>\n",
       "      <td>158.483822</td>\n",
       "      <td>143.132806</td>\n",
       "      <td>120.706959</td>\n",
       "      <td>2</td>\n",
       "      <td>5.669881</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaee</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>35.487000</td>\n",
       "      <td>-79.062133</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>133.678329</td>\n",
       "      <td>94.498573</td>\n",
       "      <td>160.707916</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>7.386471</td>\n",
       "      <td>0.002955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaff</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>111825.0</td>\n",
       "      <td>38.049471</td>\n",
       "      <td>-99.827001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>693.751953</td>\n",
       "      <td>685.274170</td>\n",
       "      <td>726.738892</td>\n",
       "      <td>...</td>\n",
       "      <td>229.735649</td>\n",
       "      <td>188.723331</td>\n",
       "      <td>146.595467</td>\n",
       "      <td>0.102113</td>\n",
       "      <td>205.936458</td>\n",
       "      <td>168.080443</td>\n",
       "      <td>128.281072</td>\n",
       "      <td>6</td>\n",
       "      <td>11.624690</td>\n",
       "      <td>0.027916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aafl</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2017313.0</td>\n",
       "      <td>39.474744</td>\n",
       "      <td>-86.898353</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>199.500000</td>\n",
       "      <td>199.068390</td>\n",
       "      <td>271.521088</td>\n",
       "      <td>...</td>\n",
       "      <td>238.247501</td>\n",
       "      <td>242.393052</td>\n",
       "      <td>239.020702</td>\n",
       "      <td>0.252126</td>\n",
       "      <td>243.033961</td>\n",
       "      <td>245.751794</td>\n",
       "      <td>243.296854</td>\n",
       "      <td>5</td>\n",
       "      <td>14.517277</td>\n",
       "      <td>0.140605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid  region  severity    density   latitude  longitude        date  \\\n",
       "0  aabm       3         1      585.0  39.080319 -86.430867  2018-05-14   \n",
       "1  aacd       2         1      290.0  35.875083 -78.878434  2020-11-19   \n",
       "2  aaee       2         1     1614.0  35.487000 -79.062133  2016-08-24   \n",
       "3  aaff       3         3   111825.0  38.049471 -99.827001  2019-07-23   \n",
       "4  aafl       3         4  2017313.0  39.474744 -86.898353  2021-08-23   \n",
       "\n",
       "    elevation        mine        maxe  ...      r_1000      g_1000  \\\n",
       "0  164.000000  162.775070  245.746124  ...   -1.000000   -1.000000   \n",
       "1  112.367500   82.372543  124.047012  ...  167.504627  155.254219   \n",
       "2  133.678329   94.498573  160.707916  ...   -1.000000   -1.000000   \n",
       "3  693.751953  685.274170  726.738892  ...  229.735649  188.723331   \n",
       "4  199.500000  199.068390  271.521088  ...  238.247501  242.393052   \n",
       "\n",
       "       b_1000  prop_lake_2500      r_2500      g_2500      b_2500  cluster  \\\n",
       "0   -1.000000       -1.000000   -1.000000   -1.000000   -1.000000        5   \n",
       "1  135.638269        0.160221  158.483822  143.132806  120.706959        2   \n",
       "2   -1.000000       -1.000000   -1.000000   -1.000000   -1.000000        2   \n",
       "3  146.595467        0.102113  205.936458  168.080443  128.281072        6   \n",
       "4  239.020702        0.252126  243.033961  245.751794  243.296854        5   \n",
       "\n",
       "   logDensity  split_pred  \n",
       "0    6.371612    0.026089  \n",
       "1    5.669881    0.003110  \n",
       "2    7.386471    0.002955  \n",
       "3   11.624690    0.027916  \n",
       "4   14.517277    0.140605  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Personal functions, these should be run from ./algaebloom, see the README.md\n",
    "from src import feat, mod\n",
    "from src.single_fe import get_features, pred_out\n",
    "import pandas as pd\n",
    "\n",
    "# Show function for predicting single out of sample\n",
    "\n",
    "# Grabbing the data used for training\n",
    "train = feat.get_data(split_pred=True)\n",
    "test = feat.get_data(data_type='test') # Note that this is the submission format data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d71bac",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Some EDA notes I have. First, the spatial variation is quite substantial. In real production, I would want to more formally model this (maybe via a kriging or other type of spatial auto-regressive/spatial error model). But given the competition has spatial-hold out sets, this is not viable.\n",
    "\n",
    "A simple approach, which improved the models quite a bit, was to create an ad-hoc cluster variable, to identify cluster areas for the test set data (but identify train set close them in space). I represented this variable via the `cluster` ordinal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148a1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is how I encoded regions\n",
      "{'west': 4, 'midwest': 3, 'south': 2, 'northeast': 1}\n",
      "  severity                                             \n",
      "     count      mean       std  min  25%  50%  75%  max\n",
      "0   1143.0  1.805774  0.938980  1.0  1.0  2.0  2.0  5.0\n",
      "1   9948.0  1.567652  0.783326  1.0  1.0  1.0  2.0  5.0\n",
      "2   2200.0  2.194091  1.043424  1.0  1.0  2.0  3.0  5.0\n",
      "3   3769.0  3.747413  0.715230  1.0  4.0  4.0  4.0  5.0\n",
      "\n",
      "Here are my cluster differences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">severity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.983917</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9565.0</td>\n",
       "      <td>1.551281</td>\n",
       "      <td>0.770109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>812.0</td>\n",
       "      <td>1.837438</td>\n",
       "      <td>0.992290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>207.0</td>\n",
       "      <td>1.685990</td>\n",
       "      <td>0.732695</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507.0</td>\n",
       "      <td>1.972130</td>\n",
       "      <td>1.017388</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1044.0</td>\n",
       "      <td>2.576628</td>\n",
       "      <td>0.959821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3433.0</td>\n",
       "      <td>3.917565</td>\n",
       "      <td>0.321922</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  severity                                             \n",
       "     count      mean       std  min  25%  50%  75%  max\n",
       "0    492.0  1.666667  0.983917  1.0  1.0  1.0  2.0  5.0\n",
       "1   9565.0  1.551281  0.770109  1.0  1.0  1.0  2.0  5.0\n",
       "2    812.0  1.837438  0.992290  1.0  1.0  2.0  3.0  5.0\n",
       "3    207.0  1.685990  0.732695  1.0  1.0  2.0  2.0  4.0\n",
       "4   1507.0  1.972130  1.017388  1.0  1.0  2.0  3.0  5.0\n",
       "5   1044.0  2.576628  0.959821  1.0  2.0  3.0  3.0  5.0\n",
       "6   3433.0  3.917565  0.321922  1.0  4.0  4.0  4.0  5.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show region/cluster variability\n",
    "print('Here is how I encoded regions')\n",
    "print(feat.reg_ord)\n",
    "reg_stats = train.groupby(['region'],as_index=False)[['severity']].describe()\n",
    "print(reg_stats)\n",
    "\n",
    "print(\"\\nHere are my cluster differences\")\n",
    "clus_stats = train.groupby(['cluster'],as_index=False)[['severity']].describe()\n",
    "clus_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc17999",
   "metadata": {},
   "source": [
    "One additional thing I want to show, is that there are significant differences across *days of the week*. Clearly this does not matter for mitosis, so this suggests some type of human based biased in the data. It would be useful to set a randomized set of days for data collection going forward in the future, as this is clearly an artificial thing that may work well for the competition, but will not likely work well in real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9552b75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">severity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3332.0</td>\n",
       "      <td>2.111044</td>\n",
       "      <td>1.060595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5407.0</td>\n",
       "      <td>2.588496</td>\n",
       "      <td>1.320560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4837.0</td>\n",
       "      <td>1.824271</td>\n",
       "      <td>1.069023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2853.0</td>\n",
       "      <td>1.899755</td>\n",
       "      <td>1.046823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>380.0</td>\n",
       "      <td>2.336842</td>\n",
       "      <td>1.328483</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>131.0</td>\n",
       "      <td>1.786260</td>\n",
       "      <td>1.109453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>120.0</td>\n",
       "      <td>1.783333</td>\n",
       "      <td>1.022300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  severity                                             \n",
       "     count      mean       std  min  25%  50%  75%  max\n",
       "0   3332.0  2.111044  1.060595  1.0  1.0  2.0  3.0  5.0\n",
       "1   5407.0  2.588496  1.320560  1.0  1.0  3.0  4.0  5.0\n",
       "2   4837.0  1.824271  1.069023  1.0  1.0  1.0  3.0  5.0\n",
       "3   2853.0  1.899755  1.046823  1.0  1.0  2.0  3.0  5.0\n",
       "4    380.0  2.336842  1.328483  1.0  1.0  2.0  4.0  5.0\n",
       "5    131.0  1.786260  1.109453  1.0  1.0  1.0  2.5  4.0\n",
       "6    120.0  1.783333  1.022300  1.0  1.0  1.0  3.0  4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show differences across day of week (mitosis)\n",
    "\n",
    "# 0 is Monday, 7 is Sunday\n",
    "\n",
    "bt = pd.to_datetime('1/1/2015')\n",
    "diff_days, week_day, month, year = mod.dummy_stats(train['date'],begin_date=bt)\n",
    "train['week_day'] = week_day\n",
    "week_stats = train.groupby(['week_day'],as_index=False)[['severity']].describe()\n",
    "week_stats  # Tuesdays/Fridays are worse!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92e267",
   "metadata": {},
   "source": [
    "## Train/Test Validation Approach\n",
    "\n",
    "So because the test data is totally different lat/lon coordinates (spatial hold outs), it was important to identify the differences in the two samples. Using this, I was able to use a hyper-tuning strategy that much better approximated the real test data using in-sample data.\n",
    "\n",
    "The variable `split_pred`, in the above dataset was generated via predicing the probability a sample is in the test set based on the variables `latitude,longitude,maxe,dife,region`. Here I show generating a second example with the same approach as in `get_split.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe10e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>split_pred</th>\n",
       "      <th>split_pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabm</td>\n",
       "      <td>0.026089</td>\n",
       "      <td>0.026089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aacd</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaee</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.002955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaff</td>\n",
       "      <td>0.027916</td>\n",
       "      <td>0.027916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aafl</td>\n",
       "      <td>0.140605</td>\n",
       "      <td>0.140605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid  split_pred  split_pred2\n",
       "0  aabm    0.026089     0.026089\n",
       "1  aacd    0.003110     0.003110\n",
       "2  aaee    0.002955     0.002955\n",
       "3  aaff    0.027916     0.027916\n",
       "4  aafl    0.140605     0.140605"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show pred training\n",
    "all_dat = feat.get_both() # this is the combined train/test data\n",
    "\n",
    "# Predicting probability of in test set\n",
    "cm = mod.CatMod(ord_vars=['region'],\n",
    "                ide_vars=['latitude','longitude','maxe','dife'],\n",
    "                y='test')\n",
    "cm.fit(all_dat)\n",
    "\n",
    "# Predict probability in the train set\n",
    "split_pred2 = cm.predict(train)\n",
    "train['split_pred2'] = split_pred2\n",
    "train[['uid','split_pred','split_pred2']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b789c66",
   "metadata": {},
   "source": [
    "Using a typical train/test approach, I get much too optimistic results. Here I do not use weights to determine the train/test splits. (Under the hood this does random test sizes of 2000 and does 10 random train/test splits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb79882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count      mean       std       min       25%       50%       75%  \\\n",
      "AvgError    10.0  0.651469  0.029104  0.605766  0.635764  0.651449  0.661734   \n",
      "midwest     10.0  0.724845  0.034729  0.659656  0.701418  0.731358  0.754869   \n",
      "northeast   10.0  0.741491  0.091785  0.613941  0.675947  0.767488  0.784505   \n",
      "south       10.0  0.755994  0.011216  0.740193  0.746156  0.756982  0.762152   \n",
      "west        10.0  0.383546  0.031974  0.340047  0.362920  0.382151  0.403413   \n",
      "\n",
      "                max  \n",
      "AvgError   0.715457  \n",
      "midwest    0.761183  \n",
      "northeast  0.909718  \n",
      "south      0.775365  \n",
      "west       0.437079  \n"
     ]
    }
   ],
   "source": [
    "# Show off differences in train/test with no weights\n",
    "\n",
    "lig = mod.RegMod(ord_vars=['region','cluster','imtype'],\n",
    "                dat_vars=['date'],\n",
    "                ide_vars=['latitude','longitude','elevation','dife'],\n",
    "                y='severity',\n",
    "                weight='split_pred2',\n",
    "                mod = mod.LGBMRegressor(n_estimators=1000,max_depth=12)\n",
    "                )\n",
    "\n",
    "# Waaaay too optimistic, overall average MSE 0.65\n",
    "met_noweights = lig.met_eval(train,pr=True,ret=True,weight=False,cat=False,full_train=False,split_tt='not_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972afbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count      mean       std       min       25%       50%       75%  \\\n",
      "AvgError    10.0  0.798991  0.019836  0.765394  0.788205  0.795500  0.811945   \n",
      "midwest     10.0  0.832061  0.027888  0.791447  0.816570  0.829026  0.846358   \n",
      "northeast   10.0  0.797087  0.037476  0.742641  0.772045  0.797776  0.809876   \n",
      "south       10.0  0.798426  0.026542  0.744133  0.782781  0.804539  0.819512   \n",
      "west        10.0  0.768390  0.041207  0.707966  0.752090  0.768277  0.787684   \n",
      "\n",
      "                max  \n",
      "AvgError   0.829185  \n",
      "midwest    0.890946  \n",
      "northeast  0.856819  \n",
      "south      0.829156  \n",
      "west       0.840987  \n"
     ]
    }
   ],
   "source": [
    "# Using the same model, but with weights for the train/test split evaluations\n",
    "# is much more realistic compared to actual leaderboard\n",
    "\n",
    "# This model has an average MSE of 0.79, much closer to real life (this one is overfit)\n",
    "met_weights = lig.met_eval(train,pr=True,ret=True,weight=True,cat=False,full_train=False,split_tt='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f65920",
   "metadata": {},
   "source": [
    "In tinkering with the leaderboard, here is the final set of ensembled models. I tested with individual models up until January 4th. Individual models have at best RMSE's around 0.8 (best one was 0.78 with a very simple xgboost model without satellite data). But ensembling the models together resulted in a drop to 0.76. Then I just tinkered with a few things (iterations, depth, changing single variables) to drop down below 0.75 over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608f6d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of predictions for test set\n",
      "2    2512\n",
      "4    2091\n",
      "1    1241\n",
      "3     666\n",
      "Name: severity, dtype: int64\n",
      "\n",
      "This shows differences in current model vs best results on 2/16\n",
      "0    6510\n",
      "Name: dif_2023_02_16, dtype: int64\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Showing off fitting the final model\n",
    "\n",
    "sat_1000 = ['prop_lake_1000', 'r_1000', 'g_1000', 'b_1000']\n",
    "sat_2500 = ['prop_lake_2500', 'r_2500', 'g_2500', 'b_2500']\n",
    "sat_1025 = sat_1000 + sat_2500\n",
    "\n",
    "cat = mod.RegMod(ord_vars=['region','cluster'],\n",
    "                dat_vars=['date'],\n",
    "                ide_vars=['latitude','longitude','maxe','dife'],\n",
    "                y='severity',\n",
    "                mod = mod.CatBoostRegressor(iterations=380,depth=6,\n",
    "                   allow_writing_files=False,verbose=False)\n",
    "                )\n",
    "cat.fit(train,weight=False,cat=False)\n",
    "\n",
    "\n",
    "lig = mod.RegMod(ord_vars=['region','cluster','imtype'],\n",
    "                dat_vars=['date'],\n",
    "                ide_vars=['latitude','longitude','elevation','dife'] + sat_1025,\n",
    "                y='severity',\n",
    "                mod = mod.LGBMRegressor(n_estimators=470,max_depth=8)\n",
    "                )\n",
    "lig.fit(train,weight=False,cat=True)\n",
    "\n",
    "\n",
    "xgb = mod.RegMod(ord_vars=['region','cluster'],\n",
    "                 dat_vars=['date'],\n",
    "                 y='severity',\n",
    "                 mod = mod.XGBRegressor(n_estimators=70, max_depth=2))\n",
    "xgb.fit(train,weight=False,cat=False)\n",
    "\n",
    "\n",
    "rm = mod.EnsMod(mods={'xgb': xgb, 'cat': cat, 'lig': lig})\n",
    "\n",
    "\n",
    "# To generate predictions for the test set\n",
    "test['pred'] = rm.predict_int(test)\n",
    "form_dat = feat.sub_format(test)\n",
    "print('Distribution of predictions for test set')\n",
    "print(form_dat['severity'].value_counts())\n",
    "\n",
    "# Showing it is equivalent to other saved model\n",
    "current = form_dat.copy()\n",
    "print('\\nThis shows differences in current model vs best results on 2/16')\n",
    "print(mod.check_day(current,day=\"sub_2023_02_16.csv\")) # should all be 0's if the model is the same\n",
    "\n",
    "\n",
    "# To save the final sample\n",
    "form_dat.to_csv(f'sub_notebook.csv',index=False)\n",
    "\n",
    "# And to save the model if you want\n",
    "# default saves to the ./model folder\n",
    "mod.save_model(rm,f'mod_notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5a914",
   "metadata": {},
   "source": [
    "In terms of model interpretability, this is quite low. It is an ensembled model of many different boosted trees, so can be highly non-linear and factor in many different variables. Here is a quick view of each models normalized feature importance (need to see the documents for each model, as they are all slightly different)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c1cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catboost Feature Importance\n",
      "            Var        FI\n",
      "0       cluster  0.174895\n",
      "1     longitude  0.166795\n",
      "2      latitude  0.157506\n",
      "3          maxe  0.125172\n",
      "4     days_date  0.117256\n",
      "5          dife  0.104067\n",
      "6    month_date  0.094194\n",
      "7        region  0.035100\n",
      "8  weekday_date  0.025013\n",
      "\n",
      "Lightboost Feature Importance\n",
      "               Var        FI\n",
      "0        elevation  0.157693\n",
      "1        days_date  0.150518\n",
      "2             dife  0.126096\n",
      "3         latitude  0.113921\n",
      "4        longitude  0.106747\n",
      "5       month_date  0.053917\n",
      "6   prop_lake_1000  0.051960\n",
      "7   prop_lake_2500  0.043699\n",
      "8           r_1000  0.039858\n",
      "9           b_1000  0.028698\n",
      "10          g_1000  0.028408\n",
      "11          r_2500  0.026886\n",
      "12          b_2500  0.023915\n",
      "13          g_2500  0.021741\n",
      "14    weekday_date  0.016233\n",
      "15         cluster  0.007392\n",
      "16          imtype  0.001667\n",
      "17          region  0.000652\n",
      "\n",
      "XGBoost Feature Importance\n",
      "            Var        FI\n",
      "0       cluster  0.889193\n",
      "1    month_date  0.052532\n",
      "2        region  0.041813\n",
      "3     days_date  0.012272\n",
      "4  weekday_date  0.004190\n"
     ]
    }
   ],
   "source": [
    "print('Catboost Feature Importance')\n",
    "print(cat.feat_import())\n",
    "\n",
    "print('\\nLightboost Feature Importance')\n",
    "print(lig.feat_import())\n",
    "\n",
    "print('\\nXGBoost Feature Importance')\n",
    "print(xgb.feat_import())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab52ac",
   "metadata": {},
   "source": [
    "Note because only one of these models has satellite imagery data, the solution is mostly just fitting a curve based on dates over time and spatial variation. In practice this model is not likely to have much more discriminative ability than to just say \"water areas in this area at this time tend to have these concentrations\".\n",
    "\n",
    "It is very difficult to create a competition that effectively allows one to build a model that can be actually used in production. I think it would need to be submitting a function, which can leverage prior nearby samples in space-time to generate predictions, and you apply it to totally new samples in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8cd0f",
   "metadata": {},
   "source": [
    "## Showing Prediction for a single new observation\n",
    "\n",
    "This is to illustrate how to make a prediction for a single piece of data given lat/lon/date/region and piping in a particular saved model. So here I show for the model built in this exact notebook in earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5ce782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector and prediction from original model\n",
      "[36.5597, -121.51, '2016-08-31', 'west', 4]\n",
      "\n",
      "Prediction grabbing data right now and using local model\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Show prediction\n",
    "\n",
    "aabn = test[['latitude','longitude','date','region','pred']].head(1).values.tolist()[0] #aabn\n",
    "\n",
    "# The function expects region to be the original string\n",
    "rev_ord = {v:k for k,v in feat.reg_ord.items()}\n",
    "aabn[3] = rev_ord[aabn[3]]\n",
    "print('Vector and prediction from original model')\n",
    "print(aabn)\n",
    "\n",
    "print('\\nPrediction grabbing data right now and using local model')\n",
    "print(pred_out(aabn[0],aabn[1],aabn[2],aabn[3],rm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34de5fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the dynamically generated feature values\n",
      "cluster: 7\n",
      "imtype: 1\n",
      "elevation: 30.869382858276367\n",
      "dife: 21.346538543701172\n",
      "maxe: 49.59114074707031\n",
      "prop_lake_1000: 0.24529027297193387\n",
      "r_1000: 153.4611089341693\n",
      "g_1000: 116.86539968652038\n",
      "b_1000: 79.04202586206897\n",
      "prop_lake_2500: 0.27616292560679423\n",
      "r_2500: 151.63212805287924\n",
      "g_2500: 119.83840746134887\n",
      "b_2500: 77.18650571364553\n",
      "\n",
      "Here is the original feature values in the cached dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imtype</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elevation</th>\n",
       "      <td>30.869383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dife</th>\n",
       "      <td>21.346539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxe</th>\n",
       "      <td>49.591141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_lake_1000</th>\n",
       "      <td>0.245098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_1000</th>\n",
       "      <td>155.874804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g_1000</th>\n",
       "      <td>120.173824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_1000</th>\n",
       "      <td>83.218529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop_lake_2500</th>\n",
       "      <td>0.275378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_2500</th>\n",
       "      <td>154.154273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g_2500</th>\n",
       "      <td>123.138319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_2500</th>\n",
       "      <td>81.460558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "cluster           7.000000\n",
       "imtype            1.000000\n",
       "elevation        30.869383\n",
       "dife             21.346539\n",
       "maxe             49.591141\n",
       "prop_lake_1000    0.245098\n",
       "r_1000          155.874804\n",
       "g_1000          120.173824\n",
       "b_1000           83.218529\n",
       "prop_lake_2500    0.275378\n",
       "r_2500          154.154273\n",
       "g_2500          123.138319\n",
       "b_2500           81.460558"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show saving values, and show it is the same as single row\n",
    "vars_tocheck = ['cluster','imtype','elevation','dife','maxe'] + sat_1025\n",
    "\n",
    "res_vals = get_features(aabn[0],aabn[1],aabn[2],aabn[3])\n",
    "print('Here is the dynamically generated feature values')\n",
    "for v in vars_tocheck:\n",
    "    print(f'{v}: {res_vals[v]}')\n",
    "\n",
    "print('\\nHere is the original feature values in the cached dataset')\n",
    "vars_tocheck = ['cluster','imtype','elevation','dife','maxe'] + sat_1025\n",
    "test[vars_tocheck].head(1).T\n",
    "\n",
    "# I sometimes get minor differences, I presume this could be due to the init\n",
    "# for the k-means functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e03da",
   "metadata": {},
   "source": [
    "## Duan Smearing predicting continous outcomes\n",
    "\n",
    "I think you should be predicting the continuous outcomes, and then can reduce them into bins if you want (or make a smarter threshold determination). So I think you should predict the density directly, not predict the intermediate reduced set of 1 to 5 rankings. \n",
    "\n",
    "I wrote code to predict the logDensity, and then put them back into the original density scale using [Duan's smearing](https://andrewpwheeler.com/2021/02/19/transforming-predicted-variables-in-regression/). \n",
    "\n",
    "This ended up being worse though in the competition than just predicing the integer severities 1-5 though in my experiments. So I include here just to show it off. (Hit me up NASA/NOAA if you need some stat consulting!) Not shown here, you may also want to [generate prediction intervals](https://andrewpwheeler.com/2022/02/04/prediction-intervals-for-random-forests/), and only flag if the low end of the interval is above some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31bc60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>pred</th>\n",
       "      <th>predDens</th>\n",
       "      <th>logDens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabn</td>\n",
       "      <td>4</td>\n",
       "      <td>1.438460e+07</td>\n",
       "      <td>15.472339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aair</td>\n",
       "      <td>4</td>\n",
       "      <td>3.597007e+06</td>\n",
       "      <td>14.086283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aajw</td>\n",
       "      <td>2</td>\n",
       "      <td>3.998125e+04</td>\n",
       "      <td>9.586837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalr</td>\n",
       "      <td>3</td>\n",
       "      <td>1.303134e+06</td>\n",
       "      <td>13.070954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aalw</td>\n",
       "      <td>4</td>\n",
       "      <td>1.954046e+06</td>\n",
       "      <td>13.476083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamp</td>\n",
       "      <td>3</td>\n",
       "      <td>1.043011e+06</td>\n",
       "      <td>12.848293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aapj</td>\n",
       "      <td>4</td>\n",
       "      <td>3.074452e+06</td>\n",
       "      <td>13.929308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aaqf</td>\n",
       "      <td>1</td>\n",
       "      <td>1.635582e+04</td>\n",
       "      <td>8.693010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aauy</td>\n",
       "      <td>1</td>\n",
       "      <td>1.190777e+04</td>\n",
       "      <td>8.375617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aava</td>\n",
       "      <td>1</td>\n",
       "      <td>6.737680e+02</td>\n",
       "      <td>5.503557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid  pred      predDens    logDens\n",
       "0  aabn     4  1.438460e+07  15.472339\n",
       "1  aair     4  3.597007e+06  14.086283\n",
       "2  aajw     2  3.998125e+04   9.586837\n",
       "3  aalr     3  1.303134e+06  13.070954\n",
       "4  aalw     4  1.954046e+06  13.476083\n",
       "5  aamp     3  1.043011e+06  12.848293\n",
       "6  aapj     4  3.074452e+06  13.929308\n",
       "7  aaqf     1  1.635582e+04   8.693010\n",
       "8  aauy     1  1.190777e+04   8.375617\n",
       "9  aava     1  6.737680e+02   5.503557"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lig_logDens = mod.RegMod(ord_vars=['region','cluster','imtype'],\n",
    "                         dat_vars=['date'],\n",
    "                         ide_vars=['latitude','longitude','elevation','dife'] + sat_1025,\n",
    "                         y='density',\n",
    "                         transform=mod.safelog,\n",
    "                         inv_trans=mod.np.exp,\n",
    "                         mod = mod.LGBMRegressor(n_estimators=500,max_depth=8)\n",
    "                         )\n",
    "\n",
    "lig_logDens.fit(train,weight=False,cat=True)\n",
    "\n",
    "test['predDens'] = lig_logDens.predict(test,duan=True)\n",
    "test['logDens'] = lig_logDens.predict(test,duan=False)\n",
    "\n",
    "# pred is the integer prediction, \n",
    "# predDens is the density prediction\n",
    "# logDens is the prediction on log scale\n",
    "test[['uid','pred','predDens','logDens']].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloom_jpy",
   "language": "python",
   "name": "bloom_jpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
