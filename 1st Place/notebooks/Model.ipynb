{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.neighbors import KDTree\n",
    "import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "files = glob.glob('../downloaded_data/nrrr/*.csv')\n",
    "for f in files:\n",
    "    dfs.append(pd.read_csv(f, index_col=0))\n",
    "temp = pd.concat(dfs)\n",
    "\n",
    "temp = temp[~temp.id.duplicated()]\n",
    "\n",
    "df = pd.read_csv('../inputs/metadata.csv')\n",
    "train_label = pd.read_csv('../inputs/train_labels.csv')\n",
    "\n",
    "train_df = df[df.split == 'train'].copy()\n",
    "train = train_df.merge(train_label, on='uid', how='inner')\n",
    "\n",
    "train['month'] = pd.DatetimeIndex(train.date).month\n",
    "train['year'] = pd.DatetimeIndex(train.date).year\n",
    "\n",
    "train = train.merge(temp, left_on='uid', right_on='id', how='inner').drop(labels='id', axis=1)\n",
    "\n",
    "test_df = df[df.split == 'test'].copy()\n",
    "test_meta = pd.read_csv('../inputs/submission_format.csv')\n",
    "\n",
    "test_df = test_df.merge(test_meta, on='uid')\n",
    "\n",
    "test_df['month'] = pd.DatetimeIndex(test_df.date).month\n",
    "test_df['year'] = pd.DatetimeIndex(test_df.date).year\n",
    "\n",
    "test = test_df.merge(temp, left_on='uid', right_on='id', how='inner').drop(labels='id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## water_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_color = pd.read_csv('../outputs/Sentinels_available_features.csv')\n",
    "\n",
    "train = train.merge(water_color[\n",
    "    ['uid', 'R', 'G', 'B', 'GMax', 'GMin', 'G_R', 'G_B', 'R_B', 'GMax_B', 'GMin_B']\n",
    "], how='left', on='uid')\n",
    "\n",
    "test = test.merge(water_color[\n",
    "    ['uid', 'R', 'G', 'B', 'GMax', 'GMin', 'G_R', 'G_B', 'R_B', 'GMax_B', 'GMin_B']\n",
    "], how='left', on='uid')\n",
    "\n",
    "train_full = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'latitude', 'longitude', 'date', 'split', 'region', 'severity',\n",
       "       'density', 'month', 'year', 'Visibility', 'Wind speed (gust)',\n",
       "       'Surface pressure', 'Orography', 'Temperature',\n",
       "       'Plant canopy surface water',\n",
       "       'Water equivalent of accumulated snow depth (deprecated)', 'Snow cover',\n",
       "       'Snow depth', 'Percent frozen precipitation', 'Precipitation rate',\n",
       "       'Categorical snow', 'Categorical ice pellets',\n",
       "       'Categorical freezing rain', 'Categorical rain', 'Surface roughness',\n",
       "       'Frictional velocity', 'Instantaneous surface sensible heat flux',\n",
       "       'Latent heat net flux', 'Ground heat flux', 'Vegetation Type',\n",
       "       'Convective available potential energy', 'Convective inhibition',\n",
       "       'Downward short-wave radiation flux',\n",
       "       'Downward long-wave radiation flux', 'Upward short-wave radiation flux',\n",
       "       'Upward long-wave radiation flux', 'Visible Beam Downward Solar Flux',\n",
       "       'Visible Diffuse Downward Solar Flux', 'Boundary layer height',\n",
       "       'Land-sea mask', 'Sea ice area fraction', 'Lightning', 'Vegetation',\n",
       "       'unknown', 'Leaf Area Index', 'Cloud Forcing Net Solar Flux', 'R', 'G',\n",
       "       'B', 'GMax', 'GMin', 'G_R', 'G_B', 'R_B', 'GMax_B', 'GMin_B'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full.head().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'learning_rate': 0.005,\n",
    "          'bagging_fraction': 0.3,\n",
    "          'feature_fraction': 0.3,\n",
    "          'min_split_gain': 0.1,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 2023\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['region', 'month', 'Visibility', 'Wind speed (gust)',\n",
       "       'Surface pressure', 'Orography', 'Temperature',\n",
       "       'Plant canopy surface water',\n",
       "       'Water equivalent of accumulated snow depth (deprecated)', 'Snow cover',\n",
       "       'Snow depth', 'Percent frozen precipitation', 'Precipitation rate',\n",
       "       'Categorical snow', 'Categorical ice pellets',\n",
       "       'Categorical freezing rain', 'Categorical rain', 'Surface roughness',\n",
       "       'Frictional velocity', 'Sensible heat net flux', 'Latent heat net flux',\n",
       "       'Ground heat flux', 'Vegetation Type',\n",
       "       'Convective available potential energy', 'Convective inhibition',\n",
       "       'Downward short-wave radiation flux',\n",
       "       'Downward long-wave radiation flux', 'Upward short-wave radiation flux',\n",
       "       'Upward long-wave radiation flux', 'Visible Beam Downward Solar Flux',\n",
       "       'Visible Diffuse Downward Solar Flux',\n",
       "       'Planetary boundary layer height', 'Land-sea mask',\n",
       "       'Sea ice area fraction', 'Lightning', 'Vegetation', 'unknown',\n",
       "       'Leaf Area Index', 'Cloud Forcing Net Solar Flux', 'R', 'G', 'B',\n",
       "       'GMax', 'GMin', 'G_R', 'G_B', 'R_B', 'GMax_B', 'GMin_B'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.958168\n",
      "Early stopping, best iteration is:\n",
      "[540]\tvalid_0's rmse: 0.951754\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.865246\n",
      "Early stopping, best iteration is:\n",
      "[577]\tvalid_0's rmse: 0.855562\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.833155\n",
      "[2000]\tvalid_0's rmse: 0.835086\n",
      "Early stopping, best iteration is:\n",
      "[1526]\tvalid_0's rmse: 0.832569\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.88101\n",
      "Early stopping, best iteration is:\n",
      "[506]\tvalid_0's rmse: 0.878805\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.847846\n",
      "[2000]\tvalid_0's rmse: 0.844041\n",
      "Early stopping, best iteration is:\n",
      "[2138]\tvalid_0's rmse: 0.84342\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.842564\n",
      "Early stopping, best iteration is:\n",
      "[808]\tvalid_0's rmse: 0.839871\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Early stopping, best iteration is:\n",
      "[430]\tvalid_0's rmse: 0.786232\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.909529\n",
      "Early stopping, best iteration is:\n",
      "[1157]\tvalid_0's rmse: 0.908297\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.901928\n",
      "Early stopping, best iteration is:\n",
      "[1141]\tvalid_0's rmse: 0.900704\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[1000]\tvalid_0's rmse: 0.872445\n",
      "Early stopping, best iteration is:\n",
      "[518]\tvalid_0's rmse: 0.866838\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../outputs/weights'):\n",
    "    os.makedirs('../outputs/weights')\n",
    "\n",
    "test_pred_list = []\n",
    "rmses = []\n",
    "models = []\n",
    "for i in range(100):\n",
    "    # northeast', 'midwest\n",
    "    train = train_full[train_full.region.isin(['northeast', 'midwest'])].copy()\n",
    "    km = KMeans(n_clusters=100)\n",
    "    train['cluster'] = km.fit_predict(train[['longitude', 'latitude']].values).astype(str)\n",
    "    train['fold'] = -1\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    for idx, (trn, val) in enumerate(gkf.split(train, groups=train.cluster)):\n",
    "        train.iloc[val, -1] = idx\n",
    "    verbose_eval = 1000\n",
    "    num_rounds = 30000\n",
    "    early_stop = 500\n",
    "    test_preds = []\n",
    "\n",
    "    oofs = []\n",
    "    for f in range(5):\n",
    "        trn_data = train[(train.fold != f)].drop(\n",
    "            labels=['latitude', 'longitude', 'year', 'cluster', 'fold', 'uid', 'date', 'split', 'severity', 'density'], axis=1).copy()\n",
    "        trn_label = train[(train.fold != f)].severity\n",
    "        val_data = train[(train.fold == f)].drop(\n",
    "            labels=['latitude', 'longitude', 'year', 'cluster', 'fold', 'uid', 'date', 'split', 'density'], axis=1).copy()\n",
    "        test_data = test.drop(['latitude', 'longitude', 'year', 'uid', 'date', 'split', 'severity'], 1).copy()\n",
    "\n",
    "        trn_data['region'] = trn_data['region'].map({\n",
    "            'midwest': 0,\n",
    "            'south': 1,\n",
    "            'northeast': 2,\n",
    "            'west': 3\n",
    "        })\n",
    "\n",
    "        val_data['region'] = val_data['region'].map({\n",
    "            'midwest': 0,\n",
    "            'south': 1,\n",
    "            'northeast': 2,\n",
    "            'west': 3\n",
    "        })\n",
    "\n",
    "        test_data['region'] = test_data['region'].map({\n",
    "            'midwest': 0,\n",
    "            'south': 1,\n",
    "            'northeast': 2,\n",
    "            'west': 3\n",
    "        })\n",
    "\n",
    "\n",
    "        d_train = lgb.Dataset(trn_data, label=trn_label.values, categorical_feature=['region'])\n",
    "        d_valid = lgb.Dataset(val_data.drop(labels='severity', axis=1),\n",
    "                              label=val_data.severity, categorical_feature=['region'])\n",
    "\n",
    "\n",
    "        model = lgb.train(lgb_params, d_train, num_boost_round=num_rounds, valid_sets=d_valid,\n",
    "                             early_stopping_rounds=early_stop, verbose_eval=verbose_eval)\n",
    "\n",
    "        val_pred = model.predict(val_data.drop(labels='severity', axis=1))\n",
    "        val_data['pred'] = np.round(val_pred).astype(np.int)\n",
    "    #     val_data['pred'] = [pseudo_round(x) for x in val_pred]\n",
    "        val_data['raw_pred'] = val_pred\n",
    "\n",
    "        test_pred = model.predict(test_data)\n",
    "        test_preds.append(test_pred)\n",
    "        oofs.append(val_data)\n",
    "        \n",
    "        model.save_model(f'../outputs/weights/model_i{i}_f{f}.bin')\n",
    "\n",
    "    oof = pd.concat(oofs)\n",
    "    rmses.append(np.sqrt(mean_squared_error(oof.severity, oof.pred)))\n",
    "    test_pred_list.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9171332678538724\n"
     ]
    }
   ],
   "source": [
    "test_pred_flat = []\n",
    "for e in test_pred_list:\n",
    "    test_pred_flat.extend(e)\n",
    "    \n",
    "test['severity'] = np.stack(test_pred_flat, -1).mean(1)#).astype(int)\n",
    "lgb_test = test[['uid', 'region', 'severity']].copy()\n",
    "\n",
    "print(np.mean(rmses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pred rest test based on knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdt = KDTree(train_full[['latitude', 'longitude']].values, leaf_size=30, metric='euclidean')\n",
    "distance, matches = kdt.query(test_df[['latitude', 'longitude']].values, k=100, return_distance=True)\n",
    "pred = []\n",
    "for i, x in enumerate(matches):\n",
    "    pred.append((train_full.iloc[x].severity * (1 / distance[i])).sum() / (1 / distance[i]).sum())\n",
    "#     pred.append(pseudo_round(train.iloc[x].severity.mean()))\n",
    "#        pred.append(trn.iloc[x].severity.value_counts().index[0])\n",
    "test_df['severity'] = pred\n",
    "knn_test = test[['uid', 'region', 'severity']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../outputs/weights/kdt.bin', 'wb') as fp:\n",
    "    pickle.dump(kdt, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_test = pd.concat([lgb_test[lgb_test.region.isin(['midwest', 'northeast'])],\n",
    "                   knn_test[~knn_test.region.isin(['midwest', 'northeast'])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_round(x):\n",
    "    if x < 1.65:\n",
    "        return 1\n",
    "    elif x < 2.55:\n",
    "        return 2\n",
    "    elif x < 3.5:\n",
    "        return 3\n",
    "    elif x < 4.5:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_test['severity'] = mg_test.apply(lambda x: pseudo_round(x.severity), 1)\n",
    "test_df['severity'] = test_df.apply(lambda x: int(np.round(x.severity)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.concat([\n",
    "    test_df[~test_df.uid.isin(mg_test[mg_test.region.isin(['northeast', 'midwest'])].uid)][\n",
    "        ['uid', 'region', 'severity']], mg_test[mg_test.region.isin(['northeast', 'midwest'])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2281\n",
       "4    2222\n",
       "1    1500\n",
       "3     507\n",
       "Name: severity, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.severity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7559\n",
    "sub.set_index('uid').loc[test_df.uid].to_csv('../outputs/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tickrep]",
   "language": "python",
   "name": "conda-env-tickrep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
